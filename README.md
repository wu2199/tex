Depth Anything V2（arXiv:2406.09414

I. 总体框架

* 目标：在判别式单目深度范式下，同时获得细节刻画、真实场景稳健性与推理效率。
* 三项核心实践：① 教师模型仅用高精度合成数据训练；② 显著增大教师容量；③ 用教师为海量真实无标注图像生成伪标签，供学生模型学习。
* 模型族：学生提供多尺度（约 25M–1.3B 参），编码器为 DINOv2 系列，解码器为 DPT。

II. 数据与蒸馏路径

* 教师训练数据（仅合成，约 59.5 万张）：BlendedMVS、Hypersim、IRS、TartanAir、VKITTI2。动机：获得稠密且精确的几何监督，避免真实深度传感器噪声。
* 学生训练数据（真实伪标，约 6200 万张原始图像）：BDD100K、Google Landmarks、ImageNet-21K、LSUN、Objects365、Open Images V7、Places365、SA-1B 等，由最强教师统一生成伪深度。
* 经验结论：教师阶段混入哪怕少量真实“真标注”会削弱细节；学生阶段仅用真实伪标签即可取得更强的真实域泛化，小模型尤为明显。
* 蒸馏层级：采用“预测级/标签级”蒸馏而非特征/Logit 级，以避免师生容量差过大导致的对齐不稳定。

III. 模型与训练目标

* 架构：DINOv2 编码器 + DPT 解码器；训练时短边统一到 518 后随机裁剪。
* 监督形式：仿射不变的逆深度。
* 损失设计（合成标注阶段）：

  * 尺度与偏移不变的对齐损失：对预测与标注先做最优仿射对齐，再计算误差，抵消单目尺度漂移。
  * 深度梯度匹配损失：在图像平面对一阶梯度匹配，强化几何边缘与薄结构；在“精确稠密”合成标注下收益显著，梯度项权重提升可稳定增强锐度。
* 损失设计（伪标签阶段）：增加特征对齐项以保留 DINOv2 的语义表征；对单图“最难 10% 区域”做忽略掩码以抑制伪标签噪声。
* 训练流程：教师（纯合成）→ 教师为真实大库打伪标 → 学生（仅伪标）多尺度训练。

IV. 同时提升细节与稳健性的原因

* 细节：合成标注的高精度 + 梯度匹配，使网络学习到像素级几何边界；真实标注的噪声/稀疏会“钝化”该能力，故教师阶段坚持纯合成。
* 稳健：覆盖广泛的真实图像分布通过伪标签蒸馏注入学生，缓解合成→真实域偏移；对复杂布局、透明/反射等场景鲁棒性明显增强。
* 分辨率性质：测试期上调分辨率（超出训练的 518）可近似单调提升边缘锐度，无需额外训练。

V. 推理与工程特性

* 效率：前馈式判别模型，无扩散过程与采样调度；较扩散式深度管线显著更快，参数更小。
* 部署建议：在显存/时延允许下尽可能上调测试分辨率；根据资源选用 S/B/L/G 尺度学生模型。

VI. 实证对比

* 零样本相对深度：在传统基准上数值不弱于 V1 并优于 MiDaS；作者强调旧基准对薄结构与透明场景的改进反映不足。
* 新基准 DA-2K：覆盖八类场景，标注更可靠；最小学生模型亦可在多场景超越社区代表方法（如生成式方案），透明/反射与细薄结构中优势显著。
* 度量深度微调：在 NYU-D、KITTI 等场景可进一步提升；优先采用合成数据进行度量监督以避免真实标注噪声。

VII. 数据清单与可复现性

* 教师训练（合成 595K）：BlendedMVS 115K、Hypersim 60K、IRS 103K、TartanAir 306K、VKITTI2 20K。
* 学生训练（真实伪标 62M）：BDD100K 8.2M、Google Landmarks 4.1M、ImageNet-21K 13.1M、LSUN 9.8M、Objects365 1.7M、Open Images V7 7.8M、Places365 6.5M、SA-1B 11.1M。
* 训练日程、损失权重与消融、分辨率放大、透明性压力测试等在正文与附录给出，便于复现。

VIII. 与 DA-V1 与扩散式方法的关系

* 相比 V1：性能提升主要来自“数据与蒸馏路径重构”（纯合成强教师 + 大规模真实伪标学生），非架构更换。
* 相比扩散式：避免扩散/去噪的推理开销，同时在报告的对比中实现更高吞吐与精度，更适合大规模实际部署。

IX. 关键消融结论

* 梯度匹配损失对边缘锐度的贡献依赖于“精确稠密”标签，在合成监督下最有效。
* 在教师阶段混入真实标注会显著降低细节；学生阶段仅用真实伪标反而更佳，尤其对小模型。
* 测试期分辨率上调是一种稳定、低成本的细节增益手段。


==========

Marigold（arXiv:2505.09358v1）

—

一、研究动机与总体思路

1. 目标：以最小代价将预训练扩散式图像生成器（Stable Diffusion 等 LDM）“适配”为密集图像分析器（单目深度、表面法线、内在图像分解），在小数据、低算力下获得具竞争力的零样本泛化。([ar5iv][1])
2. 核心思想（Marigold-Depth 首先提出并在本文扩展）：
   a) 复用 LDM 的 VAE，将输入图像与目标模态（如深度/法线/内在分量）共同编码到同一潜空间；
   b) 仅使用高质量合成数据做短程微调；
   c) 训练生成的是条件分布 p(y|x) 而非直接回归其“众数”，推理时通过条件扩散采样并做“测试时集成（多次采样聚合）”；
   d) 通过调度器设置与蒸馏，显著降低推理步数与函数评估次数（NFE）。([ar5iv][1])

—

二、方法总览与训练/推理机制

1. 条件扩散建模：以输入图像的 VAE 潜变量 z_x 作为条件，将目标模态的潜变量 z_y 作为被建模的随机变量，使用 U-Net 在潜空间执行去噪（与 Stable Diffusion 一致），损失函数为标准扩散训练（噪声回归）。([ar5iv][1])
2. 推理与集成：给定 z_x，按 DDIM 调度进行少步采样得到若干预测，再在像素或潜域聚合（均值/近邻一致化等）；作者评估显示，集成数目从 1→10 带来稳定提升，超过 10 收益递减。([ar5iv][1])
3. 加速与单步化：
   a) 采用 DDIM“trailing timesteps”与 zero-SNR 设置可将性能峰值推理步数降到 1 步（v1.1 默认），显著降低 NFE；
   b) 进一步引入 Latent Consistency Distillation（LCM），将多步 DDPM 参数化蒸馏为一致性函数，实现单步或极少步推理，同时保持与基线相当的精度。([ar5iv][1])

—

三、单目深度（Marigold-Depth）

1. 训练与数据：完全基于合成数据（如 HyperSim、Virtual KITTI），短程微调协议；v1.1 在数据增广（翻转、模糊、色抖动）与调度器（DDIM trailing + zero-SNR）上做了修订。([ar5iv][1])
2. 评测与指标：遵循仿射不变深度评测协议，报告 AbsRel 与 δ-阈值准确率等；在多数据集零样本评测（NYUv2、KITTI、ETH3D、ScanNet、DIODE）下，相对同等训练成本方法取得更优或具竞争力结果。([ar5iv][1])
3. 效率：推理步数可降至 1；表中给出与他法的推理时长对比，显示少步与集成结合后的效率—精度权衡。([ar5iv][1])

—

四、表面法线（Marigold-Normals）

1. 适配改动：遵循深度微调协议，直接将三通道法线图作为 VAE 编码的目标；训练时沿通道归一化以保证单位长度约束；其余管线与调度与深度相仿。([ar5iv][1])
2. 结果与对比：在 ScanNet、NYUv2、iBims-1、DIODE、OASIS 等基准上，对比多种判别式/生成式基线（DSINE、StableNormal、Lotus 系列等），Marigold-Normals 在低数据/低 NFE 设定下总体更优或具有竞争力。([ar5iv][1])
3. 集成与步数消融：与深度类似，集成数提升带来单调增益，最佳 DDIM 步数常在 4 左右；视觉上 1 步即可给出合理结果，高步数提升高频细节但不必然提升数值指标。([ar5iv][1])

—

五、内在图像分解（IID）：材料与照明两条模型

1. 任务定义：
   a) Marigold-IID-Appearance：输出反照率（albedo）与材料双参（粗糙度/金属性，编码到 RG 通道）；
   b) Marigold-IID-Lighting：输出反照率、漫反射阴影与非漫反射残差（线性空间下的分解）。([ar5iv][1])
2. 架构变更：U-Net 首层与末层按输出图像数量复制通道并做权值缩放以维持激活统计，其余保持与 LDM 一致；训练分别在 InteriorVerse、HyperSim 等合成数据上进行。([ar5iv][1])

—

六、单步蒸馏：Latent Consistency Models（LCM）

1. 原理：将 DDPM 轨迹上的点映射到同一输出的一致性函数进行学习（改变参数化以缩短推理轨迹），使 LDM 在潜空间中实现一到数步收敛；
2. 结论：一跳 LCM 并非在所有基准全面超越多步 DDIM，但多数数据集与指标上超越既有他法，验证了 Marigold 对一致性蒸馏的可适配性。([ar5iv][1])

—

优点、适用性与局限

1. 优点：
   a) 统一、可复用的潜空间条件生成范式，适配多种密集任务；
   b) 仅需合成数据的小样本微调即可获得强零样本泛化；
   c) 通过调度器修订与 LCM 蒸馏，显著降低 NFE；
   d) 提供系统的 HR 推理策略以兼顾全局一致性与边缘质量。([ar5iv][1])
2. 局限：
   a) 测试时集成引入额外延迟（尽管 1 步推理可缓解）；
   b) 仍存在合成到真实的域差异，质量依赖合成数据多样性与保真度；
   c) HR 中的 global conditioning 带来显存压力，需要与 MultiDiffusion 折中。([ar5iv][1])

—


